{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mv2mOlen31Z"
   },
   "source": [
    "## Take Home Assessment\n",
    "\n",
    "**Disclaimer**: This assessment is work in progress, so we apologise in advance for any hiccup. Any feedback is valuable!\n",
    "\n",
    "**Setup**: You are provided with some training code for a model that takes protein 3D structure and predicts the associated amino acid sequence. This notebook provides the required steps to download the code repository and training data (a subset of the Protein Data Bank), alongside minimal code to call the training loop. Please fork the repository that you can find below and edit your own version.\n",
    "\n",
    "**Compute**: You will be provided a [Lambda](https://cloud.lambdalabs.com/) instance with a A10 GPU on an agreed day. For this we need your public key and we will share an IP address to access the compute instance.\n",
    "\n",
    "**Evaluation**: The following questions are on purpose quite open-ended. No specific answer is expected. The aim is to provide a semi-realistic setup that you may encounter if you were to join our team. We want to assess your ability to probe deep learning models and to come up with solutions to alleviate potential identified limitations. Please write down your answers (e.g. with plots, tables etc) in your copy of the repository (e.g. in this notebook or in any other format of your choice) and push them to your fork. Do include any documentation of what all you did to arrive at your answers. We will discuss during the onsite interview. Please keep the time commitment under 4h.\n",
    "\n",
    "**Questions**:\n",
    "1. Log and profile the training loop.  What would you recommend if we wanted to train more quickly? Implement some of your proposals.\n",
    "2. What kinds of issues will arise as model size increases? How could these be partially alleviated? Implement some of your proposed solutions.\n",
    "3. The way the dataloader is organized in this project is unusual.  What will happen as we increase the size of the training dataset (e.g. using the AlphaFold database)?  How would you re-organize the code to avoid these issues?  What techniques would you consider using to ensure training scales efficiently with the dataset size?\n",
    "4. Log the average norm of the weights & activations through training. How would you organize this information to help diagnose training dynamics?  How would you characterize the values you observe here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PshWHcHclaUQ",
    "outputId": "56a95b2c-0d54-4460-d8d9-38516b65f075"
   },
   "outputs": [],
   "source": [
    "# Download subset of training data\n",
    "!wget https://files.ipd.uw.edu/pub/training_sets/pdb_2021aug02_sample.tar.gz\n",
    "!tar xvf \"pdb_2021aug02_sample.tar.gz\"\n",
    "!rm pdb_2021aug02_sample.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Thm0LtbBPH4J",
    "outputId": "c6202787-e83c-41fa-a8dd-a31ecb5da82f"
   },
   "outputs": [],
   "source": [
    "from training.training import main as run_training\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "class MyArgs(object):\n",
    "  def __init__(self):\n",
    "    self.path_for_training_data = \"/tmp/content/pdb_2021aug02_sample\"\n",
    "    self.path_for_outputs = \"/tmp/content/test\"\n",
    "    self.previous_checkpoint = \"\"\n",
    "    self.num_epochs = 2\n",
    "    self.save_model_every_n_epochs = 5\n",
    "    self.reload_data_every_n_epochs = 4\n",
    "    self.num_examples_per_epoch = 200\n",
    "    self.batch_size = 2000\n",
    "    self.max_protein_length = 2000\n",
    "    self.hidden_dim = 128\n",
    "    self.num_encoder_layers = 3\n",
    "    self.num_decoder_layers = 3\n",
    "    self.num_neighbors = 32\n",
    "    self.dropout = 0.1\n",
    "    self.backbone_noise = 0.1\n",
    "    self.rescut = 3.5\n",
    "    self.debug = False\n",
    "    self.gradient_norm = -1.0 #no norm\n",
    "\n",
    "args = MyArgs()\n",
    "run_training(args)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
